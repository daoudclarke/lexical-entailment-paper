\documentclass[11pt]{article}
\usepackage{eacl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\setlength\titlebox{6.5cm}


\title{Learning to Distinguish Hypernyms and Co-Hyponyms Using Distributional Information and Support Vector Machines}
\author{Julie Weeds, Daoud Clarke etc.}
\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}
\section{Introduction}

Over recent years there has been much interest in the field of distributional semantics, drawing on the distributional hypothesis ---  words that occur in similar contexts tend to have similar meanings \cite{Harris1954}.   There is a vast body of work on the use of different similarity measures \cite{Lee1999,Weeds2003,Curran2004} and many researchers have automatically built thesauruses, i.e., lists of ``nearest neighbours'' and applied them generally with a good deal of success in a variety of applications.

In the early days there was a strong interest in how these automatically generated thesauruses compare with human-constructed gold standards such as WordNet and Roget \cite{Lin1998,Kilgarriff2000}.  More recently, the focus has tended to shift to building domain-specific thesauruses to alleviate the sparse-data problem in a variety of tasks such as sentiment classification \cite{Bollegala2011}.  

However, primarily the focus has always been on ``similarity'' - identifying words which are similar to each other. However, semantic similarity encompasses a number of different relations.  Even if we consider only nouns, an automatically generated thesaurus will tend to return a mix of synonyms, antonyms, hyponyms, hypernyms, co-hyponyms, meronyms and other topically related words.  Some of these relations are symmetric whereas others are not.  For example, hyponymy, and its converse hypernymy, is the backbone of the majority of taxonomies and ontologies such as WordNet.  We can say a \texttt{cat} ISA \texttt{animal}  but we cannot say an \texttt{animal} ISA \texttt{cat}.  This ISA relation, which we can think of  as describing a lexical entailment\cite{Geffet2005}  between two words, is asymmetric.  However, many semantically related words are in co-hyponyms, that is, they share a common ancestor in the ontology.  Since \texttt{cat} ISA \texttt{animal} and \texttt{dog} ISA \texttt{animal}, we would say that \texttt{cat} and \texttt{dog} are related by co-hyponymy.  Co-hyponymy, synonymy and antonymy are all symmetric relations 

\begin{table}[ht]
\begin{tabular}{|c|c|c|}
\hline
cat&animal&dog\\
\hline
dog 0.32&bird 0.36&cat 0.32\\
animal 0.29&fish 0.34&animal 0.32\\
rabbit 0.27&creature 0.33&horse 0.29\\
bird 0.26&dog 0.31&bird 0.26\\
bear 0.26&horse 0.30&rabbit 0.26\\
monkey 0.26&insect 0.30&pig 0.25\\
mouse 0.25&species 0.29&bear 0.26\\
pig 0.25&cat 0.29&man 0.25\\
snake 0.24&human 0.28&fish 0.24\\
horse 0.24&mammal 0.28&boy 0.24\\
rat 0.24&cattle 0.27&creature 0.24\\
elephant 0.23&snake 0.27&monkey 0.24\\
tiger 0.23&pig 0.26&snake 0.24\\
deer 0.23&rabbit 0.26&mouse 0.24\\
creature 0.23&elephant 0.25&rat 0.23\\
wolf 0.23&sheep 0.25&girl 0.23\\
lion 0.23&bear 0.25&elephant 0.23\\
dragon 0.23&plant 0.25&wolf 0.23\\
fish 0.22&deer 0.24&woman 0.22\\
man 0.22&food 0.24&deer 0.22\\
\hline
\end{tabular}
\label{table:neighbours}
\caption{Top 20 neighbours of \texttt{cat}, \texttt{animal} and \texttt{dog} generated using Lin's similarity measure \cite{Lin1998} considering all words and dependency features occurring 100 or more times in Wikipedia}
\end{table}
Most, but not all, measures of distributional similarity assume that similarity is a symmetric relation and it follows that $sim(A,B) = sim(B,A)$.  For example, Table \ref{table:neighbours} shows the distributionally nearest neighbours of the words \texttt{cat}, \texttt{animal} and \texttt{dog} using the Byblo software and Wikipedia.  In the list for \texttt{cat} we can see 2 hypernyms and 18 co-hyponyms.  Given a similarity score of 0.32 between cat and animal, how do we know which is the hyponym and which is the hypernym? However, there are times when we would want to know the hyponyms of a word and other times when we would want to know the hypernyms of a word.  For example, consider the following two sentences.

\begin{itemize}
\item[A]{The cat ran across the road.}
\item[B]{The animal ran across the road.}
\end{itemize}

Sentence A textually entails sentence B but B does not textually entail A.  We believe that the ability to be able to automatically distinguish different semantic relations using distributional information, in particular those concerned with lexical entailment, is crucial if distributional semantics is to be used successfully in systems which are concerned with the meaning of phrases and sentences.

In this work we show that it is possible to train a support vector machine (SVM) to identify pairs of words $\langle A,B\rangle$ where a lexical entailment holds from word A to word B from other pairs where there may be an entailment in the other direction, some other semantic relation or no relation at all.  The perfomance of this supervised method is far superior to the existing state-of-the-art, which tend to draw on the notion of distributional generality as described by \cite{Weeds2004}, used either in an unsupervised or weakly supervised setting.  We are also able to train an SVM to identify co-hyponym pairs from other pairs where there is some other semantic relation (hypernymy, hyponymy or meronymy) or no relation at all.  We also experiment with vectors drawn from different sources and show that the quality of the vectors has significant impact on the performance of the classifier.

Part of our motivation for this work is that we are interested in the
problem of how to compose distributional representations of meaning,
which has recently received a lot of interest
\cite{Widdows:08,Mitchell:08,Baroni2010,Grefenstette:11,Socher:12}. Our
hope is that we can eventually extend our system to determine
entailment between phrases and sentences.

\section{Related Work}

Weeds, Weir and McCarthy \shortcite{Weeds2004} proposed a notion of distributional generality, observing that more general words tend to occur in a larger variety of contexts than more specific words.  In our example, we would expect to be able to replace any occurrence of \texttt{cat} with \texttt{animal} and so all of the contexts of \texttt{cat} must be plausible contexts for \texttt{animal}.  However, not all of the contexts of \texttt{animal} would be plausible for \texttt{cat}, e.g., ``the monstrous animal barked at the intruder''.  Weeds, Weir and McCarthy attempt to capture this asymmetry by framing word similarity in terms of co-occurrence retrieval \cite{Weeds2003}, where precision and recall are defined as:

\[
P_{ww}(u,v) = \frac{\Sigma_{f \in F(u) \cap F(v)} I(u,f)}{\Sigma_{f \in F(u)} I(u,f)}
\]

\[
R_{ww}(u,v) = \frac{\Sigma_{f \in F(u) \cap F(v)} I(v,f)}{\Sigma_{f \in F(v)} I(v,f)}
\]

where $I(n,f)$ is the PPMI between noun $n$ and feature $f$ and F(n) is the set of all features $f$ for which $I(n,f)>0$

By comparing the precision and recall of one word's retrieval of another word's contexts, they were able to successfully identify the direction of an entailment relation in 71\% of pairs drawn from WordNet.  However, this was not signifcantly better than a baseline which proposed that the most frequent word was the most general.

Clarke \shortcite{Clarke2007} formalised the idea of distributional generality using a partially ordered vector space.  He also argued for using a variation of co-occurrence retrieval where precision and recall are defined as:

\[
P_{cl}(u,v) = \frac{\Sigma_{f \in F(u) \cap F(v)} min(I(u,f),I(v,f))}{\Sigma_{f \in F(u)} I(u,f)}
\]

\[
R_{cl}(u,v) = \frac{\Sigma_{f \in F(u) \cap F(v)} min(I(u,f),I(v,f))}{\Sigma_{f \in F(v)} I(v,f)}
\]

\newcite{Lenci2012} took the notion further and hypothesised that more general terms should have high recall and low precision, which would thus make it possible to distinguish them from other related terms such as synonyms and co-hyponyms.  They proposed a variant of the \newcite{Clarke2007} measure to identify hypernyms:

\[
invCL(u,v) = \sqrt[2]{P_{cl}(u,v)*(1-R_{clarke}(u,v))}
\]

On an evaluation using the BLESS dataset \cite{Baroni2011}, they showed this measure was significantly better at distinguishing hypernyms from other semantic relations than the measures of Weeds, Weir and McCarthy \shortcite{Weeds2004} and Clarke \shortcite{Clarke2007}.

Geffet and Dagan \shortcite{Geffet2005} adopted an approach, referred to as \emph{feature inclusion} which extends the rationale of \newcite{Weeds2004} to the lexical entailment setting.    Using data collected from the web they were able to demonstrate a strong correlation between the complete inclusion of prominent features and lexical entailment.  However, they were unable to assess this using an off-line corpus due to data sparseness.

Szpektor and Dagan \shortcite{Szpektor2008} found that the $P_{ww}$ tends to promote relationships between infrequent words with narrow templates (i.e., ones with relatively few distinct context features).  They proposed using the geometric average of $P_{ww}$ and the symmetric Lin similarity measure in order to penalise low frequency words.   

Kotlerman et al. \shortcite{Kotlerman2010} provide a good overview of using directional measures of distributional similarity to perform lexical inference.  In particular, they propose adapting a common IR evalation method, \emph{Average Precision} to the problem of identifying lexical inference and using the balancing approach of Szpektor and Dagan \shortcite{Szpektor2008} to demote similarities for narrow feature vectors.  They show that all of the asymmetric similarity measures previously proposed perform much better than symmetric similarity measures on a directionality detection experiment and that their method outperforms the others with statistical significance.

The Stanford WordNet project \cite{Snow:04} expands the WordNet
taxonomy by analysing large corpora to find patterns that are
indicative of hyponymy. For example, the pattern ``$\mathit{NP}_X$ and
other $\mathit{NP}_Y$'' is an indication that $\mathit{NP}_X$ is a
$\mathit{NP}_Y$, i.e.~that $\mathit{NP}_X$ is a hyponym of
$\mathit{NP}_Y$. They use machine learning to identify other such
patterns from known hyponym-hypernym pairs, and then use these
patterns to find new relations in the corpus. They enforce the
transitivity relation of the taxonomy by only searching over valid
taxonomies, and evaluating the likelihood of each taxonomy given the
available evidence \cite{Snow:06}. Whilst this approach is similar to
ours in that it is a supervised method of learning semantic relations,
it relies on having features for occurrences of pairs of terms,
whereas ours relies only on vectors for terms themselves, making it
more amenable to our planned application composing distributional
representations of meaning.

Also of note, Mikolov et al. \shortcite{Mikolov2013} propose a vector offset method to capture syntactic and semantic regularities between word representations learnt by a recurrent neural network language model.

\section{Theory}
\label{sect:theory}
Support vector machines (SVMs) have been shown to be an extremely effective machine learning tool in a large number of classification problems.  Given positive and negative examples of vectors, a boundary can be learnt between two classes and applied to unseen vectors.  For any given task, the SVM effectively learns which features are most important and which are noise.  For example, it would be straightforward to train an SVM to learn to classifiy documents as to whether they are in the \texttt{animal} topic class using the documents' words as features.     

Here, we have distributional vectors for two words $v_A$ and $v_B$ and we want to establish whether a given semantic relation holds between words $A$ and $B$ or not.  Whether the semantic relation holds or not corresponds to be in or out of the class in the SVM setting.  However, we have two vectors rather than one and it is unclear what operation should be performed on these vectors before training the SVM.  We have explored a number of different options as follows:

\begin{itemize}
\item[DIFF]{$v_{diff} = v_A - v_B$.  If the vectors are identical then this is the zero vector.  Further, one would expect similar vectors to have small difference vectors.  It is an asymmetric measure since $v_A - v_B = -(v_B - v_A)$ }
\item[MULT]{$v_{mult} = v_A * v_B$ using pointwise multiplication.  This essentially establishes the intersection of the two vectors since $v_{mult}[i] = 0$ for a non-shared feature $i$.   Intuitively,  dissimilar vectors will have small multiplication vectors and similar vectors will have large multiplication vectors.  It is a symmetric measure since multiplication is commutative $v_A * v_B = v_B * v_A$ }
\item[ADD]{$v_{add} = v_A + v_B$.  This emphasizes the intersection of the two vectors whilst retaining information about all features in the union of the two vectors.  It is a symmetric measure sunce addition is commutative $v_A + v_B = v_B + v_A$}
\item[CAT]{$v_{cat} = [v_A,v_B]$.  This is the concatenation of the two vectors, also referred to as the direct sum.  It retains all of the information from both vectors including the direction.    $[v_A,v_B] \neq [v_B,v_A]$}
\end{itemize}

\section{Methodology}

We have constructed two different data sets from the BLESS dataset \cite{Baroni2011}.  The BLESS dataset provides examples of hypernyms, co-hyponyms, meronyms and random unrelated words for each of 200 concrete, largely monosemous nouns.

\begin{itemize}
\item[ent]{For each noun in the BLESS dataset, 80\% of the hypernyms were randomly selected to provide positive examples of entailment.  The remaining hypernyms for the given noun were reversed and taken with the co-hyponyms, meronyms and random words to form negative examples of entailment.  A filter was applied to ensure that duplicate pairs were not included (e.g., [\texttt{cat},\texttt{animal},1] and [\texttt{animal},\texttt{cat},0] is disallowed). The result was a set of 1976 labelled pairs of nouns.}
\item[coord]{For each noun in the BLESS dataset, the co-hyponyms were taken as positive examples of this relation.  The same total number (split evenly between) of hypernyms, meronyms and random words was taken to form the negative examples.  The order of 50\% of the pairs was reversed and again duplicate pairs were disallowed, resulting in a set of 5835 labelled pairs of nouns.}
\end{itemize}

In both cases the pairs are labelled as positive or negative for the specified semantic relation and in both cases there are equal ($+-1$) numbers of positive and negative examples.

Distributional information was collected about all of the nouns from Wikipedia provided they had occurred 100 or more times in that corpus.  The features used were all grammatical dependency features (also occurring 100 or more times) based on the Stanford system of dependency parsing.  The value of each feature is the positive pointwise mutual information (PPMI) between the noun and the feature. The total number of noun vectors which can be harvested from Wikipedia with these parameters is 124,345 and for 99\% of the generated BLESS pairs, both nouns had associated vectors.   If a noun does not have an associated vector, the classifiers work on the reasonable assumption that these missing nouns have a zero vector.   For comparison we also generated vectors from GigaWord for nouns occurring 10 or more times in that corpus.  Here 13\% of the generated BLESS pairs contain at least one noun without an associated vector.  Random vectors were also constructed in order to be able to assess the impact of using distributional information over a system which simply learns the implied taxonomy in the training data and applies it to the test data.

We constructed linear SVMs for each of the vector operations outlined in Section \ref{sect:theory}.  We used linear SVMs for speed and simplicity since the point is to compare the different vector representations of the pairings.  For comparison, we also constructed a number of other classifiers, some of which are supervised and some of which are unsupervised or weakly supervised.  In Section \ref{sect:results} we report results for the classifiers described in Table \ref{table:classifiers}.

\begin{table}[ht]
\centering
\begin{tabular}{p{2cm} p{5.5cm}}
linsvm\_DIFF&A linear SVM trained on the vector difference $v_B - v_A$\\
linsvm\_MULT&A linear SVM trained on the pointwise product vector $v_B * v_A$\\
linsvm\_ADD&A linear SVM trained on the vector sum $v_B + v_A$\\
linsvm\_CAT&A linear SVM trained on the vector concatentation $[v_B,v_A]$\\
knn\_DIFF&The difference of the two vectors is calculated (as in linsvm\_DIFF) and the label of the test point is predicted based on the labels of the k closest points in the training data. k is optimised between 1 and 50.\\
widthdiff&If B's vector representation is \textit{wider} (i.e., B has more distinct features) than A's vector representation then A is predicted to entail B. \footnote{We also implemented a weakly supervised version where B's vector representation need to be wider by more than a learnt parameter but this did not improve results}\\
cosineP&If the cosine similarity of A and B's vector representations (using PPMI) is greater than a learnt parameter, A is predicted to entail B.\footnote{We also implemented the Lin similarity neasure \cite{Lin1998} but this performed consistently a few percentage points worse than cosine using PPMI.} \\
CRdiff&If $P_{ww}(A,B) > R_{ww}(A,B)$, then A is predicted to entail B.  \footnote{We also implemented a weakly supervised version where a parameter was learnt for the difference between precision and recall but this did not improve results.}\\
clarkediff&The \cite{Clarke2007} variation of CRDiff using $P_{cl}$ and $R_{cl}$\\
invCLP&Based on \cite{Lenci2012}, if invCL(A,B) is greater than a learnt parameter, A is predicted to entail B.\\
\end{tabular}
\caption{Implemented classifiers}
\label{table:classifiers}
\end{table}

\section{Results}
\label{sect:results}
Fivefold cross-validation was carried out using each combination of data set, vector set and classifier.  For each combination, the average and error for accuracy, precision, recall and F-score were recorded over the five test runs.  Due to the number of different results, these are summarised and compared in a number of precision-recall graphs.  Figures \ref{fig:entclass}  and \ref{fig:coordclass} compare results for all of the classifiers using the Wikipedia vector set on the \emph{ent} data set and the \emph{coord} data set respectively.  Figures \ref{fig:entvector} and \ref{fig:coordvector} compare result using the different vector sets for a smaller number of classifiers on the \emph{ent} data set and the \emph{coord} data set respectively.  The legend states the test-set, vector set and classifier used to generate each result.   The results for the best performing combinations are given in full in Tables \ref{table:resultsent} and \ref{table:resultscoord}.  It is worth noting that random guessing would yield an accuracy and precision of 0.5.

\begin{table*}[ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Classifier&Vectors&Acc(Error)&Precision(Error)&Recall(Error)&F-Score(Error)\\
\hline
linsvm\_CAT&Wiki&0.972(0.004)&0.965(0.007)&0.980(0.004)&0.972(0.004)\\
linsvm\_DIFF&Wiki&0.938(0.010)&0.908(0.014)&0.976(0.006)&0.940(0.009)\\
linsvm\_CAT&GW&0.900(0.004)&0.882(0.010)&0.913(0.011)&0.900(0.005)\\
knn\_DIFF&Wiki&0.868(0.006)&0.834(0.015)&0.915(0.017)&0.873(0.008)\\
\hline
cosineP&Wiki&0.582(0.018)&0.566(0.023)&0.722(0.018)&0.633(0.017)\\
invCLP&Wiki&0.607(0.010)&0.619(0.019)&0.561(0.018)&0.587(0.013)\\
linsvm\_CAT&random&0.542(0.016)&0.580(0.019)&0.410(0.108)&0.423(0.088)\\
\hline
\end{tabular}
\caption{Key results for the \emph{ent} data set}
\label{table:resultsent}
\end{table*}

\begin{table*}[ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Classifier&Vectors&Acc(Error)&Precision(Error)&Recall(Error)&F-Score(Error)\\
\hline
linsvm\_MULT&Wiki&0.952(0.002)&0.961(0.002)&0.943(0.004)&0.951(0.002)\\
linsvm\_ADD&Wiki&0.942(0.002)&0.921(0.003)&0.966(0.005)&0.943(0.002)\\
linsvm\_CAT&Wiki&0.902(0.003)&0.871(0.004)&0.944(0.007)&0.906(0.003)\\
linsvm\_ADD&GW&0.860(0.005)&0.842(0.007)&0.887(0.011)&0.863(0.005)\\
\hline
cosineP&Wiki&0.770(0.004)&0.760(0.005)&0.788(0.008)&0.773(0.004)\\
invCLP&Wiki&0.711(0.005)&686(0.005)&0.780(0.005)&0.730(0.004)\\
linsvm\_CAT&random&0.502(0.004)&0.502(0.004)&0.700(0.078)&0.576(0.027)\\
\hline
\end{tabular}
\caption{Key results for the \emph{coord} data set}
\label{table:resultscoord}
\end{table*}

\begin{figure}[ht]
\centering
%\includegraphics[scale=0.1]{graphs/entclassifiers.png}
\caption{Classifier performance using the Wikipedia vector set on the \emph{ent} data set}
\label{fig:entclass}
\end{figure}

\begin{figure}[ht]
\centering
%\includegraphics[scale=0.1]{graphs/entvectors.png}
\caption{Vector set comparison for the \emph{ent} data set}
\label{fig:entvector}
\end{figure}

\begin{figure}[ht]
\centering
%\includegraphics[scale=0.1]{graphs/coordclassifiers.png}
\caption{Classifier performance using the Wikipedia vector set on the \emph{coord} data set}
\label{fig:coordclass}
\end{figure}

\begin{figure}[ht]
\centering
%\includegraphics[scale=0.1]{graphs/coordvectors.png}
\caption{Vector set comparison for the \emph{coord} dataset}
\label{fig:coordvector}
\end{figure}

As might be expected, training linear SVMs on this task vastly outperforms unsupervised or weakly supervised techniques.  Using the cosine measure to predict that all pairs of words with a cosine over a certain threshold have the given relation yields fairly high recall on both data sets.  This is because most of the semantically related pairs have similar vectors.  However, precision is lower, particularly on the entailment task, because cosine cannot distinguish between the different relations.  The asymmetric invClarke method, which has previously been shown to increase the prominence of hypernyms over co-hyponyms in neighbour lists, does have higher precision on the entailment task but also much lower recall.  This suggests that in this implementation there are many hyponym-hypernym pairs for which the premises of high precision and low recall are not true.

Overall, the best performing SVM is the one trained on concatenation of the two vectors.  This achieves an accuracy of 97\% on the entailment task and 90\% on the coordinate task when using the vectors extracted from Wikipedia.  This is vastly superior to the state-of-the-art unsupervised techniques which achieve accuracies in the region of 60\% and 75\% on those tasks.  It is not surprising that doubling the dimensionality and retaining all of the information about both words is able to do well on both tasks.  It is probably more interesting that this performance is matched by the vector difference model at the entailment task and beaten by the vector product and vector sum models at the coordinate task.  This suggests that the difference of the two vectors is more significant for determining an entailment relation and the intersection is more significant for determining a coordinate or co-hyponymy relation.

In general, results using the Gigaword vectors showed the same pattern but consistently lower absolute scores than when using the Wikipedia vectors.  The Wikipedia corpus is larger and more balanced so again this is not surprising.  However, it does provide us with a way of quantitatively assessing the impact of using different sources of distributional information.  For both hypernyms and co-hyponyms, both precision and recall are approximately 7-8\% lower using Gigaword rather than Wikipedia.  The major exception to this pattern was the linear SVM using the vector product for the coordinate task.  This achieved the best performance using WIkipedia (95\% accuracy).  However, using Gigaword, the accuracy was 74\% with recall down at 66\%.  This suggests that the vector product model which zeros out features not in the intersection is considerably more sensitive to noise and data sparsity than the other models.

The best classifiers were able to do little better than guessing using random vectors in place of the distributional vectors.  This suggests that the power of the models is coming from the distributional vectors rather than an implicit ontological structure learnt from the training set itself.  


\section{Conclusions and Further Work}

We have shown that it is possible to very accurately predict whether there is a specific semantic relation or not between two words given distributional information about the words and training data including positive and negative examples of that relations.  It is possible to distinguish the existence of a given relationship from words which have other relationships and random non-related words.  

Training an SVM on the difference or concatenation of vectors yields best results at identifying entailment relations.  Training an SVM on the sum or product of vectors yields best results when identifying co-hyponyms.  Whilst it is not surprising that an order preserving vector operation is required to identify entailment, it is more surprising that these operations are beaten on the coordinate task.  

We have shown that the source (and hence quantity and quality) of the distributional information can have a significant impact on performance.  We have shown that SVM methods significantly outperform kNN and state-of-the-art methods based on similarity metrics. 









\bibliographystyle{acl}
\bibliography{JW2012}
\end{document}
